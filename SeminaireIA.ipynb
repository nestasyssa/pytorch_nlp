{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nestasyssa/pytorch_nlp/blob/master/SeminaireIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support seminaire IA Dauphine"
      ],
      "metadata": {
        "id": "-9kvLlSYgfHF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobjO2IPxSlp"
      },
      "source": [
        "# 1. Les images: les afficher, les transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIbRF2S1xSlq"
      },
      "source": [
        "Importation les modules nécessaires de PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kgm5a-Z3xSlq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRllF5YdxSls"
      },
      "source": [
        "## 1.1. Télécharger le fichier MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ_31lCfxSls",
        "outputId": "8f13a089-4a30-47e0-cf68-d32a6e5457e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([1, 28, 28])\n",
            "Label: 7\n"
          ]
        }
      ],
      "source": [
        "# Chemin où vous voulez télécharger les données MNIST\n",
        "data_path = './data'\n",
        "\n",
        "\n",
        "# Définir une transformation pour convertir l'image en tenseur\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Téléchargement et chargement de l'ensemble de données MNIST\n",
        "mnist_testset = datasets.MNIST(root=data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "# Accéder à une seule image et son étiquette\n",
        "image, label = mnist_testset[0]\n",
        "\n",
        "# Afficher la forme du tenseur d'image et l'étiquette\n",
        "print(\"Image shape:\", image.shape)\n",
        "print(\"Label:\", label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUcjWk4JxSlt"
      },
      "source": [
        "## 1.2. Afficher une image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IrEBBGmxSlu"
      },
      "source": [
        "Pour afficher une image, importer `mathplotlib`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "ifVUJ6sRxSlu",
        "outputId": "baad20b9-59b6-40b4-d835-9ebc3178f00a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARN0lEQVR4nO3df6xXdf3A8dfne6/zXkQzGzesXUERnS0deG1BGPde87oLehsV3SFraT+wOWY/nGyZS+61qIX9GtmQ+sPc8mJZxCZ59Sop2tCmhmWOigtqmt2SH5oFpML5/vEdry9XSO/5wAWEx2Nj03PP63PeH6afJ+97rsdKURRFAEBE/M/BXgAAhw5RACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAqX96Ec/ikqlEpVKJX7961/v8fWiKKKxsTEqlUpcdNFFg762a+5b3/rWf33dRx55JI91dXVFpVKJjRs3Djr39ttvj+bm5mhoaIgRI0bEKaecEp2dnXHnnXdGRERLS0te6/V+dXV1/df3eemll8bIkSPL/NbAm17twV4Ab151dXXR09MT55577qDjq1atimeffTaOPvro/zp7/fXXx+WXXx4jRowofd1vfvObMW/evGhubo6rr746RowYEf39/XHPPffErbfeGu3t7XHNNdfEpz/96Zx5+OGHY9GiRfGlL30pzjjjjDx+1llnlb4+HM5EgapNnz49brvttli0aFHU1v7/P0o9PT3R1NS0x5/ud5kwYUI89thjceONN8aVV15Z6pqvvvpqfOUrX4m2trbo6+vb4+v/+Mc/IiKira1t0PG6urpYtGhRtLW1RUtLS6lrwpHEt4+o2sUXXxybNm2Ku+++O4+9/PLL8bOf/Sxmz579X+emTJkS5513XixcuDC2bdtW6pobN26Mf/7znzFlypS9fr2hoaHU65U1duzYuOiii+K+++6Lc845J+rr6+PMM8+M++67LyIili1bFmeeeWbU1dVFU1NTrFmzZtD873//+7j00kvjlFNOibq6uhg9enR88pOfjE2bNu1xrV3XqKuri3HjxsWSJUvy22mv9eMf/ziampqivr4+TjjhhJg1a1Y888wzw/J7wOFNFKja2LFjY/LkybF06dI81tvbGy+++GLMmjXrdWe7urri73//eyxevLjUNRsaGqK+vj5uv/322Lx5c1Xr3lf9/f0xe/bs6OjoiK9//euxZcuW6OjoiFtuuSW+8IUvxMc+9rHo7u6O9evXR2dnZ+zcuTNn77777tiwYUN84hOfiO9973sxa9asuPXWW2P69Omx+1Ps16xZE+3t7bFp06bo7u6OT33qU3HdddfF8uXL91jPggUL4uMf/3iMHz8+vv3tb8fnP//5WLlyZUydOjVeeOGFA/A7wmGlgJJuuummIiKKhx9+uLjhhhuKY489tti6dWtRFEXx0Y9+tGhtbS2KoijGjBlTXHjhhYNmI6KYO3duURRF0draWowePTpnd3/dXebPn19ERPH888/nsWuvvbaIiOKYY44ppk2bVixYsKB49NFHX3fNt912WxERxb333jvk93nJJZcUxxxzzKBjY8aMKSKiWL16dR676667iogo6uvri6effjqPL1myZI9r7nqvu1u6dGkREcX999+fxzo6OooRI0YUf/3rX/PYunXritra2mL3f22feuqpoqampliwYMGg13z88ceL2traPY7DG7FTYJ90dnbGtm3bYsWKFfHSSy/FihUrXvdbR7vr6uqKgYGBuPHGG0tds7u7O3p6emLixIlx1113xTXXXBNNTU1x9tlnx9q1a6t5G6W8613vismTJ+ffv/e9742IiPPOOy9OOumkPY5v2LAhj9XX1+dfb9++PTZu3BiTJk2KiIjf/va3ERGxY8eOuOeee2LGjBnxjne8I88/9dRTY9q0aYPWsmzZsti5c2d0dnbGxo0b89fo0aNj/Pjxce+99+6vt80RQhTYJ6NGjYrzzz8/enp6YtmyZbFjx46YOXPmkGanTp0ara2tVd1buPjii+OBBx6ILVu2RF9fX8yePTvWrFkTHR0dsX379mreypDt/sEfEfGWt7wlIiIaGxv3enzLli15bPPmzfG5z30u3v72t0d9fX2MGjUqTj755IiIePHFFyPi/26Wb9u2LU499dQ9rv3aY+vWrYuiKGL8+PExatSoQb/Wrl2bN95hqPz0Efts9uzZMWfOnBgYGIhp06bF8ccfP+TZ+fPnR0tLSyxZsqTU3C7HHXdctLW1RVtbWxx11FFx8803x29+85tobm4u/VpDVVNTU+p4sdu9gs7Ozli9enXMmzcvJkyYECNHjoydO3dGe3v7oHsPQ7Vz586oVCrR29u71+v77ywoSxTYZx/60IfiM5/5TDz00EPxk5/8pNRsc3NztLS0xDe+8Y249tpr92kd55xzTtx8883xt7/9bZ9eZ7hs2bIlVq5cGd3d3YPe67p16wad19DQEHV1ddHf37/Ha7z22Lhx46Ioijj55JPjtNNOG56Fc0Tx7SP22ciRI2Px4sXR1dUVHR0dped33Vv4wQ9+8Ibnbt26NR588MG9fq23tzciIk4//fTSazgQdv1JfvedQ0TEd7/73T3OO//882P58uXx3HPP5fH+/v58j7t8+MMfjpqamuju7t7jdYui2OuPusLrsVNgv7jkkkuqnm1ubo7m5uZYtWrVG567devWeN/73heTJk2K9vb2aGxsjBdeeCGWL18eDzzwQMyYMSMmTpxY9VqG03HHHRdTp06NhQsXxiuvvBLvfOc7o6+vL5588sk9zu3q6oq+vr6YMmVKXH755bFjx4644YYb4t3vfnc89thjed64cePiq1/9alx99dXx1FNPxYwZM+LYY4+NJ598Mn7xi1/EZZddFlddddUBfJe82YkCh4Surq5obW19w/OOP/74+OEPfxi//OUv46abboqBgYGoqamJ008/Pa6//vr47Gc/ewBWW72enp644oor4vvf/34URREXXHBB9Pb2Dvopo4iIpqam6O3tjauuuiq+/OUvR2NjY1x33XWxdu3a+OMf/zjo3C9+8Ytx2mmnxXe+853o7u6OiP+76X3BBRfEBz/4wQP23jg8VIrX7jmBQ9aMGTPiiSee2OM+BOwv7inAIeq1P6a7bt26uOOOOzy7iWFlpwCHqBNPPDGfk/T000/H4sWL4z//+U+sWbMmxo8ff7CXx2HKPQU4RLW3t8fSpUtjYGAgjj766Jg8eXJ87WtfEwSGlZ0CAMk9BQCSKACQhnxPYW//Yw8A3jyGcrfATgGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAINUe7AUcCWbOnFl6Zs6cOVVd67nnnis9s3379tIzt9xyS+mZgYGB0jMREf39/VXNAeXZKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKlSFEUxpBMrleFey2Frw4YNpWfGjh27/xdykL300ktVzT3xxBP7eSXsb88++2zpmYULF1Z1rUceeaSqOSKG8nFvpwBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgFR7sBdwJJgzZ07pmbPOOquqa61du7b0zBlnnFF65uyzzy4909LSUnomImLSpEmlZ5555pnSM42NjaVnDqRXX3219Mzzzz9feubEE08sPVONv/zlL1XNeSDe8LJTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAqhRFUQzpxEpluNfCYe6tb31rVXMTJkwoPfPoo4+WnnnPe95TeuZA2r59e+mZP//5z6Vnqnmo4gknnFB6Zu7cuaVnIiIWL15c1RwRQ/m4t1MAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEDyQDw4jH3kIx8pPfPTn/609Mwf/vCH0jOtra2lZyIiNm/eXNUcHogHQEmiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5Cmp8CbR0NBQeubxxx8/INeZOXNm6Zmf//znpWfYN56SCkApogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkGoP9gKAoZk7d27pmVGjRpWe2bJlS+mZP/3pT6VnODTZKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIFWKoiiGdGKlMtxrgSPClClTqpr71a9+VXrmqKOOKj3T0tJSeub+++8vPcOBN5SPezsFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCk2oO9ADjSTJ8+vaq5ah5ut3LlytIzDz74YOkZDh92CgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASB6IB/ugvr6+9Ex7e3tV13r55ZdLz8yfP7/0zCuvvFJ6hsOHnQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJA8JRX2wbx580rPTJw4sapr3XnnnaVnVq9eXdW1OHLZKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIFWKoiiGdGKlMtxrgYPqwgsvLD2zfPny0jP//ve/S89ERLS3t5eeeeihh6q6FoenoXzc2ykAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACDVHuwFwHB429veVnpm0aJFpWdqampKz9xxxx2lZyI83I4Dw04BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgCpUhRFMaQTK5XhXgvsVTUPnavm4XFNTU2lZ9avX196pr29vfRMtdeC3Q3l495OAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqfZgLwDeyLhx40rPVPNwu2pceeWVpWc82I5DmZ0CAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQPCWVA2bMmDFVzfX19e3nlezdvHnzSs+sWLFiGFYCB4+dAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkgficcBcdtllVc2ddNJJ+3kle7dq1arSM0VRDMNK4OCxUwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQPJAPKpy7rnnlp654oorhmElwP5kpwBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOSBeFTl/e9/f+mZkSNHDsNK9m79+vWlZ/71r38Nw0rgzcVOAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASJ6SyiHvd7/7XemZD3zgA6VnNm/eXHoGDjd2CgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASJWiKIohnVipDPdaABhGQ/m4t1MAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAECqHeqJQ3xuHgBvYnYKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKT/BVM/DXuv6EgeAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Afficher l'image\n",
        "plt.imshow(image.squeeze(), cmap='gray')  # Utilisez la fonction squeeze() pour supprimer la dimension du canal si elle est de taille 1\n",
        "plt.title('MNIST Image')\n",
        "plt.axis('off')  # Désactive les axes\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_utzl4oixSlv"
      },
      "source": [
        "## 1.3 Convertir le tenseur en image et l'enregister"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HKsoJSCvxSlv"
      },
      "outputs": [],
      "source": [
        "# Convertir le tenseur en une image PIL\n",
        "image_pil = transforms.ToPILImage()(image)\n",
        "\n",
        "# Enregistrer l'image au format JPG\n",
        "image_pil.save('mnist_image.jpg')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "MMyUKdO8xSlw",
        "outputId": "8083f6a7-1f41-4f0e-87ac-d297fe636d2e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du tenseur: torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# lire l'image et la convertir en tenseur\n",
        "image = Image. open ('mnist_image.jpg')\n",
        "transform = transforms. ToTensor()\n",
        "img_tensor = transform(image)\n",
        "\n",
        "# Afficher l'image\n",
        "plt.imshow(img_tensor.permute(1, 2, 0))  # Permutation des dimensions pour matplotlib differnte des dimensions PyTorch\n",
        "plt.title('MNIST Image')\n",
        "plt.axis('off')  # Désactiver les axes\n",
        "plt.show()\n",
        "\n",
        "# Visualisation de la taille du tenseur\n",
        "print(\"Taille du tenseur:\", img_tensor.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0c2RH_P-xSlw"
      },
      "outputs": [],
      "source": [
        "# Visualisation du tensor\n",
        "#print (img_tensor)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo0ZBXhKxSlx"
      },
      "source": [
        "# 2. Strides et Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KlfOtZJxSlx",
        "outputId": "185c9a15-64dc-4d9c-d273-4ede39eb8eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape (same padding): torch.Size([1, 64, 5, 5])\n",
            "Output shape (custom padding): torch.Size([1, 64, 4, 4])\n",
            "Output shape (no padding): torch.Size([1, 64, 2, 2])\n",
            "Output shape (full padding): torch.Size([1, 64, 4, 4])\n",
            "Filter 8 Feature Map Output:\n",
            "tensor([[[-0.1184, -0.0673, -0.3228,  0.0639, -0.3086],\n",
            "         [-0.0066, -0.3291,  0.2996, -1.1613, -0.1526],\n",
            "         [-0.3468, -0.0981,  1.4868, -0.7511,  0.6180],\n",
            "         [-1.0000,  0.2351,  0.3197, -0.8226,  0.0204],\n",
            "         [-0.2330,  0.5091,  0.4908,  0.3563,  0.4672]]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Fixer la seed pour la reproductibilité\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Supposons une entrée avec une taille de 5x5x4 et 64 filtres de taille 3x3x4\n",
        "input_size = 5\n",
        "kernel_size = 3\n",
        "in_channels = 4  # spécifier le nombre de canaux d'entrée\n",
        "out_channels = 64  # spécifier le nombre de filtres distincts\n",
        "stride_value = 2   # spécifier la valeur du stride\n",
        "\n",
        "# Créer des données d'entrée (exemple Jouet random)\n",
        "batch_size = 1\n",
        "input_data = torch.randn(batch_size, in_channels, input_size, input_size)\n",
        "\n",
        "# Créer une couche de convolution avec un padding \"same padding\"\n",
        "# Il faut calculer le padding manuellement pour maintenir la taille de l'entrée\n",
        "same_padding = (kernel_size - 1) // 2\n",
        "conv_same_padding = nn.Conv2d(in_channels, out_channels, kernel_size, padding=same_padding, stride=1)\n",
        "\n",
        "# Créer une couche de convolution avec \"Custom padding\" et un stride de 2\n",
        "padding_value = 2  # padding personnalisé\n",
        "conv_custom_padding = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding_value, stride=stride_value)\n",
        "\n",
        "# Créer une couche de convolution sans padding et un stride de 2\n",
        "conv_no_padding = nn.Conv2d(in_channels, out_channels, kernel_size, padding=0, stride=stride_value)\n",
        "\n",
        "# Créer une couche de convolution avec un padding \"full padding\" et un stride de 2\n",
        "full_padding = kernel_size - 1\n",
        "conv_full_padding = nn.Conv2d(in_channels, out_channels, kernel_size, padding=full_padding, stride=stride_value)\n",
        "\n",
        "# Passer les données à travers les couches de convolution\n",
        "output_same_padding = conv_same_padding(input_data)\n",
        "output_custom_padding = conv_custom_padding(input_data)\n",
        "output_no_padding = conv_no_padding(input_data)\n",
        "output_full_padding = conv_full_padding(input_data)\n",
        "\n",
        "# Vérifier les formes des sorties\n",
        "print(\"Output shape (same padding):\", output_same_padding.shape)\n",
        "print(\"Output shape (custom padding):\", output_custom_padding.shape)\n",
        "print(\"Output shape (no padding):\", output_no_padding.shape)\n",
        "print(\"Output shape (full padding):\", output_full_padding.shape)\n",
        "\n",
        "# Accéder aux valeurs de la carte des caractéristiques (feature maps) par exemple pour le filtre 8 et les afficher\n",
        "filter_8_output = output_same_padding[:, 8, :, :]\n",
        "print(\"Filter 8 Feature Map Output:\")\n",
        "print(filter_8_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1o55-O3xSlx"
      },
      "source": [
        "# 3. CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importation des modules\n"
      ],
      "metadata": {
        "id": "muNUcCvnyDpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "f5d0ldmJx_ul"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Initialisation Gloriot"
      ],
      "metadata": {
        "id": "u0VORT7XrISn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialisation Glorot**.  La fonction ` def init_weights_glorot(m)` est une fonction utilisée pour initialiser les poids  de manière à maintenir des gradients stables lors de la rétropropagation dans les réseaux de neurones. `m` représente un module de réseau de neurones.\n",
        "\n",
        "- `if isinstance(m, nn.Conv2d)`  vérifie si le module `m` est une couche de convolution.\n",
        "- `elif isinstance(m, nn.Linear)` vérifie si le module `m` est une couche linéaire.\n",
        "- `if m.bias is not None`  vérifie si la couche a un biais.\n",
        "- `nn.init.xavier_uniform_(m.weight.data)` initialise les poids du module `m` en utilisant la  fonction `xavier_uniform_` du module `nn.init` de PyTorch. Les poids sont accessibles via `m.weight.data`.\n",
        "- `nn.init.zeros_(m.bias.data)` initialise les biais à zéro en utilisant la fonction `zeros_` du module `nn.init` de PyTorch. Les biais sont accessibles via `m.bias.data`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mEAWSB_pyNbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation Glorot (Xavier)\n",
        "def init_weights_glorot(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias.data)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias.data)"
      ],
      "metadata": {
        "id": "IzRvVQxb152v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Initialisation He"
      ],
      "metadata": {
        "id": "_8YLai0prQat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialisation HE**.  La fonction `def init_weights_he(m)` est une fonction utilisée pour initialiser les poids  avec la méthode He. `m` représente un module de réseau de neurones.\n",
        "\n",
        "- `if isinstance(m, nn.Conv2d)`  vérifie si le module `m` est une couche de convolution.\n",
        "- `elif isinstance(m, nn.Linear)` vérifie si le module `m` est une couche linéaire.\n",
        "- `if m.bias is not None`  vérifie si la couche a un biais.\n",
        "- `nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')` initialise les poids du module `m` en utilisant la  fonction `nn.init.kaiming_uniform_` du module `nn.init` de PyTorch. Cette fonction prend en compte la non-linéarité de la fonction d'activation. Les poids sont accessibles via `m.weight.data`.\n",
        "- `nn.init.zeros_(m.bias.data)` initialise les biais à zéro en utilisant la fonction `zeros_` du module `nn.init` de PyTorch. Les biais sont accessibles via `m.bias.data`.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6P3fP5E2DPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation He\n",
        "def init_weights_he(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias.data)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias.data)"
      ],
      "metadata": {
        "id": "0FP0z3cj4HTL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3lbCJDJxSly"
      },
      "source": [
        "## 3.3. Création de la classe `CNN`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AtHCr8-xSly"
      },
      "source": [
        "**Importons les modules nécessaires de PyTorch et définissons une classe CNN.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7Tn39dxxSl1"
      },
      "source": [
        "Construisons notre modèle CNN avec deux couches de convolution (`conv1` et `conv2`) suivies de deux couches entièrement connectées (`fc1` et `fc2`) à l'aide d'une classe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3VbISRxSl1"
      },
      "source": [
        "Le code  ci-dessous définit une classe `CNN` (cf section 3.3.2.) en utilisant le module `nn.Module` de PyTorch. Cette classe représente un réseau de neurones convolutionnel (CNN) simple avec deux couches de convolution et deux couches entièrement connectées où\n",
        "\n",
        "- `class CNN(nn.Module)` définit une nouvelle classe appelée `CNN` qui hérite de `nn.Module`.\n",
        "- `def __init__(self)` définit la méthode d'initialisation de la classe `CNN`.\n",
        "- `super(CNN, self).__init__()` appelle la méthode d'initialisation de la classe parente `nn.Module`.\n",
        "- `def forward(self, x)`  définit la méthode `forward` qui spécifie comment les données sont propagées à travers le réseau.\n",
        "- `return x` renvoie la sortie finale du réseau."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, h, w, c):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * (h//4) * (w//4), 512)  # Utilisation de la taille de sortie après deux max pooling\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, 32 * (x.size(2)//4) * (x.size(3)//4))  # Calcul de la taille de l'entrée pour la couche linéaire\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x, dim=1)  # Appliquer le softmax sur les logits\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hBbuMEMiZ8Qd"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1. Précision sur le code\n",
        "\n",
        "Plus précisément, dans ce code, les commandes sont exécutées dans l'ordre ci-dessous\n",
        "\n",
        "1. **Convolution et Pooling avec `self.conv1`** :\n",
        "   - `self.conv1 = nn.Conv2d(in_channels=c, out_channels=16, kernel_size=3, stride=1, padding=1)` Définition de la couche de convolution `self.conv1`.\n",
        "   - `x = F.relu(self.conv1(x))` Application de la convolution à l'entrée `x` en utilisant `self.conv1(x)` et activation avec ReLU à la sortie de la convolution.\n",
        "   - `x = F.max_pool2d(x, kernel_size=2, stride=2)` Application d' un max pooling 2x2 et un stride de 2.\n",
        "    \n",
        "2. **Convolution et Pooling avec `self.conv2`** :\n",
        "   - `self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)` Définition de la couche de convolution  `self.conv2`.\n",
        "   - `x = F.relu(self.conv2(x))` Application de la convolution à l'entrée `x` en utilisant `self.conv2(x)` et activation avec ReLU à la sortie de la convolution.\n",
        "   -  `x = F.max_pool2d(x, kernel_size=2, stride=2)` Application d un max pooling 2x2 et un stride de 2.\n",
        "\n",
        "3. **Mise à plat des caractéristiques** :\n",
        "   - `x = x.view(-1, 32 * (x.size(2)//4) * (x.size(3)//4))` Utilisation de  `x.view()` pour aplatir les caractéristiques en ajustant la taille de la sortie précédente pour être compatible avec les couches entièrement connectées.    \n",
        "   \n",
        "   \n",
        "4. **Couches entièrement connectées** :\n",
        "   - `self.fc1 = nn.Linear(32 * (h//4) * (w//4), 512)` Définition de la couche entièrement connectée `self.fc1` avec la taille de l'entrée spécifiée.\n",
        "   - `x = F.relu(self.fc1(x))` Application de la couche entièrement connectée à l'entrée `x` en utilisant `self.fc1(x)` et activation avec ReLU à la sortie de la convolution.\n",
        "   - `self.fc2 = nn.Linear(512, 10)` Définition de la couche entièrement connectée `self.fc2` avec la taille de l'entrée et la taille de sortie spécifiée pour la classification finale.   \n",
        "   - `x = self.fc2(x)` Application de la couche entièrement connectée à l'entrée `x` en utilisant `self.fc2(x)`.\n",
        "   - `x = F.softmax(x, dim=1)` Application d'un Softmax."
      ],
      "metadata": {
        "id": "OTQUXetVtdk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2. Définition d'une classe en Python\n",
        "\n",
        " En programmation orientée objet (POO), une classe est une structure fondamentale qui regroupe des données et des méthodes agissant sur ces données. La classe `CNN` représente une architecture spécifique de réseau de neurones utilisée pour des tâches telles que la classification d'images.\n",
        "\n",
        "Les données associées à une classe sont appelées des **attributs**. Dans notre classe `CNN`, les attributs incluent différentes couches du réseau, telles que les couches de convolution (`self.conv1`, `self.conv2`), les couches entièrement connectées (`self.fc1`, `self.fc2`), etc. Ces attributs contiennent les paramètres (poids et biais) appris lors de l'entraînement du réseau.\n",
        "\n",
        "Les classes contiennent des **méthodes** qui opèrent sur les données de la classe. Dans la classe `CNN`, la méthode principale est `forward(self, x)`. Cette méthode définit comment les données sont propagées à travers le réseau. Notre classe spécifie également les opérations à effectuer sur les données à chaque couche du réseau, y compris les opérations de convolution, de pooling et les opérations linéaires.\n",
        "\n",
        "Les classes offrent un mécanisme d'**encapsulation**, permettant de regrouper les données et les méthodes qui agissent sur ces données dans une seule entité, la classe. Dans notre exemple, les différentes couches, les fonctions d'activation et les opérations sont encapsulées dans la classe `CNN`, facilitant ainsi la gestion et la manipulation du réseau.\n",
        "\n",
        "Une classe peut **hériter** des attributs et des méthodes d'une autre classe, ce qui permet d'étendre et de spécialiser les fonctionnalités. Le mécanisme d'héritage favorise la réutilisabilité du code et la hiérarchie de classes. Notre classe `CNN` hérite de la classe `nn.Module`, une classe de base dans la bibliothèque PyTorch. La classe `nn.Module` est essentielle pour la construction de réseaux de neurones et fournit de nombreuses fonctionnalités utiles pour la gestion des paramètres, la propagation avant, et d'autres aspects de l'entraînement et de l'évaluation des modèles.\n"
      ],
      "metadata": {
        "id": "_r6o9TKGld_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Création d'une instance du modèle CNN\n",
        "\n",
        "Création d'une instance du modèle CNN  et application de l'initialisation Glorot (Xavier) à ses poids à l'aide de la méthode `apply()`."
      ],
      "metadata": {
        "id": "h3lmByape03q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Par exemple pour une entrée de taille\n",
        "h=32\n",
        "w=32\n",
        "c=3\n",
        "\n",
        "model_glorot = CNN(h,w,c)\n",
        "model_glorot.apply(init_weights_glorot)\n"
      ],
      "metadata": {
        "id": "EXhK_KeJwFmT",
        "outputId": "43da298c-142b-42b7-a819-826987ca04eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeZM07M-xSl3"
      },
      "source": [
        "Création d'une autre instance du modèle CNN et application de l'initialisation He à ses poids à l'aide de la méthode `apply()`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Par exemple pour une entrée de taille\n",
        "h=28\n",
        "w=28\n",
        "c=1\n",
        "\n",
        "\n",
        "model_he = CNN(h,w,c)\n",
        "model_he.apply(init_weights_he)"
      ],
      "metadata": {
        "id": "-XnOvxKiwJPH",
        "outputId": "8ba8c7b6-3a6c-4789-c0a0-d47457af5922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=1568, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Entrainement d'un CNN :exemple"
      ],
      "metadata": {
        "id": "XvfdN2tWcOPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importation des modules nécessaires"
      ],
      "metadata": {
        "id": "lcNYAQzZcimO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "DCHEEomecnpc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définition de la transformation des données"
      ],
      "metadata": {
        "id": "qk0uf36QcqyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convertir les images PIL en tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normaliser les valeurs des pixels entre -1 et 1\n",
        "])\n"
      ],
      "metadata": {
        "id": "IaAVEStEcu7Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chargement des données MNIST"
      ],
      "metadata": {
        "id": "OVtmeS23c3ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_dataset = MNIST(root='./data', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3uyYGZAc54f",
        "outputId": "a4081a14-4889-47e5-b0dc-c254f4f6bed1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 79108653.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 38033184.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27427252.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2869487.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Division des données en ensembles d'entraînement, de validation et de test"
      ],
      "metadata": {
        "id": "je2VS5B4c8Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(mnist_dataset))\n",
        "val_size = len(mnist_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "WOKYyGDMdBQu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création des DataLoader pour charger les données en lots pendant l'entraînement"
      ],
      "metadata": {
        "id": "NJGq7ly8dM8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "L6Q11VtzdRox"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définition de la classe CNN"
      ],
      "metadata": {
        "id": "0-G41VQvdVMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 512)  # La taille de l'entrée est de 7x7 après deux max pooling\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, 32 * 7 * 7)  # Aplatir les caractéristiques pour les couches entièrement connectées\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ga8amfk_dZrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialisation Glorot (Xavier)"
      ],
      "metadata": {
        "id": "H3Ze1l1bddft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_glorot(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias.data)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias.data)"
      ],
      "metadata": {
        "id": "EgBC5Nw5dhWh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création du modèle et initialisation des poids"
      ],
      "metadata": {
        "id": "SAYrRtA9dlfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "model.apply(init_weights_glorot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMdr76RAdthd",
        "outputId": "5d73767b-94e5-4524-b326-f04b4ee638ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=1568, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définition de la fonction de perte et de l'optimiseur"
      ],
      "metadata": {
        "id": "zFgBu67fdwGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "N_aqFSI1dyuV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entraînement du modèle"
      ],
      "metadata": {
        "id": "hgukjugjd2qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQapvgPrd9jd",
        "outputId": "6b2fb15c-178e-4b7a-8618-d010d471fde1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.1842\n",
            "Epoch [2/10], Loss: 0.0514\n",
            "Epoch [3/10], Loss: 0.0351\n",
            "Epoch [4/10], Loss: 0.0235\n",
            "Epoch [5/10], Loss: 0.0181\n",
            "Epoch [6/10], Loss: 0.0139\n",
            "Epoch [7/10], Loss: 0.0118\n",
            "Epoch [8/10], Loss: 0.0098\n",
            "Epoch [9/10], Loss: 0.0095\n",
            "Epoch [10/10], Loss: 0.0051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Évaluation du modèle sur l'ensemble de validation"
      ],
      "metadata": {
        "id": "30WUpzB0eARK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy on validation set: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM5TiINGxXsV",
        "outputId": "abb6706e-77ff-424c-953a-ea63bc73e0ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on validation set: 0.9909\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}